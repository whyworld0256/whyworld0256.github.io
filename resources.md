---
layout: page
permalink: /resources/index.html
title: Resources
---

# ML/DL

- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [Andrej Karpathy's  personal page](https://karpathy.ai/)
- [Andrej Karpathy's blog](https://karpathy.github.io/)
- [Hung-yi Lee's Youtube](https://www.youtube.com/@HungyiLeeNTU)
- [colah's blog](http://colah.github.io/)

### NLP

- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [LSTM-CRF](https://www.bilibili.com/video/BV1K54y117yD?vd_source=b18932b619d828e88b5dd6a9be1a515e)
- [word embedding](https://www.youtube.com/watch?v=X7PH3NuYW0Q)
- [大语言模型核心技术-Transformer 详解 ](https://juejin.cn/post/7219249005904166949)
- [Transformer 详解](https://wmathor.com/index.php/archives/1438/)
- [理解Transformer——从一个pytorch项目出发](https://zhuanlan.zhihu.com/p/439196812)
- [PKU-TANGENT nlp-tutorial](https://github.com/PKU-TANGENT/nlp-tutorial#%E4%BB%BB%E5%8A%A1%E4%B8%80%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)
- [ELMO, BERT, GPT](https://www.youtube.com/watch?v=UYPa347-DdE&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=61)
- [BERT 的 PyTorch 实现](https://wmathor.com/index.php/archives/1457/)
- [HuggingFace-transformers系列的介绍以及在下游任务中的使用](https://www.cnblogs.com/dongxiong/p/12763923.html)
- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [Decoding Strategies in Large Language Models](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)(Greedy Search, Beam Search, Top-k sampling)
- [llm-course](https://github.com/mlabonne/llm-course)

### CV

- [Introduction to CNN / PyTorch](https://www.kaggle.com/code/chinawhy/99-5-introduction-to-cnn-w-pytorch)
- [从ViT到Swin，10篇顶会论文看Transformer在CV领域的发展历程 ](https://mp.weixin.qq.com/s?__biz=MzIyOTUyMDIwNg==&mid=2247484203&idx=1&sn=54074645fcf23fc21b4463f0a60df9af&chksm=e840250adf37ac1c37b56c4727081533080e3d89b4f1dc90f787fdc5129d4e3893ae6e3cfcce&scene=21#wechat_redirect)

### Mlsys

- [Modest Understanding on LLM](https://bytedance.larkoffice.com/docx/doxcn3zm448MK9sK6pHuPsqtH8f)
- [RISC-V Reference Card](https://whyworld0256.github.io/file/reference-card.pdf)

- [ML system 入坑指南](https://zhuanlan.zhihu.com/p/608318764)

### Paper

##### NLP

- [Neural Architectures for Named Entity Recognition](https://arxiv.org/pdf/1603.01360.pdf)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)
- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)(Transformer)
- [On the Relationship between Self-Attention and Convolutional Layers](https://arxiv.org/abs/1911.03584)
- [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236)
- [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006)
- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)
- [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)(GPT)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)(Bert)
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (GPT2)
- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)(XLNet)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)(LLaMA)

##### CV

- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)(ViT)

##### MLsys

- [Online normalizer calculation for softmax](https://arxiv.org/pdf/1805.02867v2)
- [From Online Softmax to FlashAttention](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf)
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)

# CS Online Courses

- [cmu15-123: "Introduction to Computer Systems"](http://csapp.cs.cmu.edu/)
- [cs61c: "Great Ideas in Computer Architecture"](https://cs61c.org/sp24/)
- [mit 6.s081: "Operating System Engineering"](https://pdos.csail.mit.edu/6.828/2020/index.html)
- [cmu10-714: "Deep Learning Systems ,  Algorithms and Implementation"](https://dlsyscourse.org/)
- [cs229: "Machine Learning"](https://cs229.stanford.edu/index.html-backup-fall23)
- [cs224n: "Natural Language Processing with Deep Learning"](http://web.stanford.edu/class/cs224n/)
- [cs231n: "Deep Learning for Computer Vision"](https://cs231n.stanford.edu/)
- [6.8300/6.8301: Advances in Computer Vision](https://advances-in-vision.github.io/schedule.html)
- [cs267: Applications of Parallel Computers](https://sites.google.com/lbl.gov/cs267-spr2022/home)
- [mit 6.5940: TinyML and Efficient Deep Learning Computing](https://hanlab.mit.edu/courses/2023-fall-65940)

# Parallel Computing

- [Openmp Summay Card](https://whyworld0256.github.io/file/OpenMP3.0-SummarySpec.pdf)
- [Openmp Tutorials](https://www.openmp.org/resources/tutorials-articles/)
- [MPI Tutorials](https://mpitutorial.com/tutorials/)
- [推荐几个不错的CUDA入门教程](https://godweiyang.com/2021/01/25/cuda-reading/)
- [cuda初学，有什么书籍资料推荐](https://www.zhihu.com/question/21059179)
- [有没有一本讲解gpu和CUDA编程的经典入门书籍？](https://www.zhihu.com/question/26570985/answer/3374598901)

# Books

- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slpdraft/ed3book.pdf)
- [Operating System: Three Easy Pieces](https://pages.cs.wisc.edu/~remzi/OSTEP/Homework/homework.html)

# Competitive Programming

- [OI Wiki](https://oi-wiki.org/)
- [Codeforces](https://codeforces.com/)
- [Atcoder](https://atcoder.jp/)
- [Atcoder Problems](https://kenkoooo.com/atcoder#/table/)
- [洛谷](https://www.luogu.com.cn/)
- [leetcode](https://leetcode.cn/)<br>

- [灵茶山艾府](https://github.com/EndlessCheng)
