---
layout: page
permalink: /resources/index.html
title: Resources
---

# ML/DL

- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [Andrej Karpathy's  personal page](https://karpathy.ai/)
- [Andrej Karpathy's blog](https://karpathy.github.io/)
- [Hung-yi Lee's Youtube](https://www.youtube.com/@HungyiLeeNTU)

##### NLP

- [LSTM-CRF](【【论文复现代码数据集见评论区】LSTM-CRF 知识图谱、信息抽取中最经典的论文之一，模型精讲+代码复现，你值得拥有】https://www.bilibili.com/video/BV1K54y117yD?vd_source=b18932b619d828e88b5dd6a9be1a515e)
- [大语言模型核心技术-Transformer 详解 ](https://juejin.cn/post/7219249005904166949)
- [PKU-TANGENT nlp-tutorial](https://github.com/PKU-TANGENT/nlp-tutorial#%E4%BB%BB%E5%8A%A1%E4%B8%80%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)

##### CV

- [Introduction to CNN / PyTorch](https://www.kaggle.com/code/chinawhy/99-5-introduction-to-cnn-w-pytorch)
- [从ViT到Swin，10篇顶会论文看Transformer在CV领域的发展历程 ](https://mp.weixin.qq.com/s?__biz=MzIyOTUyMDIwNg==&mid=2247484203&idx=1&sn=54074645fcf23fc21b4463f0a60df9af&chksm=e840250adf37ac1c37b56c4727081533080e3d89b4f1dc90f787fdc5129d4e3893ae6e3cfcce&scene=21#wechat_redirect)

### Paper

##### NLP

- [Neural Architectures for Named Entity Recognition](https://arxiv.org/pdf/1603.01360.pdf)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)
- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [On the Relationship between Self-Attention and Convolutional Layers](https://arxiv.org/abs/1911.03584)
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
- [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236)
- [Long Range Arena: A Benchmark for Efficient Transformers](https://arxiv.org/abs/2011.04006)
- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)
- [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)(GPT)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (GPT2)

# CS Online Courses

- [cmu15-123: "Introduction to Computer Systems"](http://csapp.cs.cmu.edu/)
- [cs61c: "Great Ideas in Computer Architecture"](https://cs61c.org/sp24/)
- [mit 6.s081: "Operating System Engineering"](https://pdos.csail.mit.edu/6.828/2020/index.html)
- [cmu10-714: "Deep Learning Systems ,  Algorithms and Implementation"](https://dlsyscourse.org/)
- [cs224n: "Natural Language Processing with Deep Learning"](http://web.stanford.edu/class/cs224n/)
- [cs231n: "Deep Learning for Computer Vision"](https://cs231n.stanford.edu/)

# Parallel Computing

- [Openmp Summay Card](https://whyworld0256.github.io/file/OpenMP3.0-SummarySpec.pdf)
- [Openmp Tutorials](https://www.openmp.org/resources/tutorials-articles/)
- [MPI Tutorials](https://mpitutorial.com/tutorials/)
- [推荐几个不错的CUDA入门教程](https://godweiyang.com/2021/01/25/cuda-reading/)
- [cuda初学，有什么书籍资料推荐](https://www.zhihu.com/question/21059179)
- [有没有一本讲解gpu和CUDA编程的经典入门书籍？](https://www.zhihu.com/question/26570985/answer/3374598901)

# Computer Architecture

- [RISC-V Reference Card](https://whyworld0256.github.io/file/reference-card.pdf)

# Competitive Programming

- [OI Wiki](https://oi-wiki.org/)
- [Codeforces](https://codeforces.com/)
- [Atcoder](https://atcoder.jp/)
- [Atcoder Problems](https://kenkoooo.com/atcoder#/table/)
- [洛谷](https://www.luogu.com.cn/)
- [leetcode](https://leetcode.cn/)<br>

- [灵茶山艾府](https://github.com/EndlessCheng)
