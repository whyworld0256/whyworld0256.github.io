# Question 1: Implementing forward computation 

In this task, we need to implement the forward computation of several operators. 

Actually, this is a relatively easy task, since we can utilize a lot of functions in numpy. Let's go through this one by one.

- `PowerScalar`: raise input to an integer (scalar) power

```c
    def compute(self, a: NDArray) -> NDArray:
        ### BEGIN YOUR SOLUTION
        return array_api.power(a, self.scalar)
        ### END YOUR SOLUTION
```

- `EWiseDiv`: true division of the inputs, element-wise (2 inputs)

```c
    def compute(self, a, b):
        ### BEGIN YOUR SOLUTION
        return array_api.divide(a, b)
        ### END YOUR SOLUTION
```

- `DivScalar`: true division of the input by a scalar, element-wise (1 input, `scalar` - number)

```c
    def compute(self, a):
        ### BEGIN YOUR SOLUTION
        return array_api.divide(a, self.scalar)
        ### END YOUR SOLUTION
```

- `MatMul`: matrix multiplication of the inputs (2 inputs)

```c
 def compute(self, a, b):
        ### BEGIN YOUR SOLUTION
        return array_api.matmul(a, b)
        ### END YOUR SOLUTION
```

- `Summation`: sum of array elements over given axes (1 input, `axes` - tuple)

before solving this function, I'd like to talk a little about the `np.sum` function， which sum the array in the given axis.

for example:

```python
a = np.array([[1,2,3],[1,2,3]])
a.shape #(2,3)
b = np.sum(a,axis=(0),keepdims=True)
b.shape #(1,3)
b = np.sum(a,axis=(1),keepdims=True)
b.shape #(2,1)
```

```python
a = np.array([[[1,2]],[[1,2]],[[1,2]]])
a.shape #(3,1,2)
b = np.sum(a,axis=(1,0),keepdims=True)
b.shape #(1,1,2)
b = np.sum(a,axis=(1,2),keepdims=True)
b.shape #(3,1,1)
b = np.sum(a,axis=(0,2),keepdims=True)
b.shape #(1,1,1)
```

You can easily tell how  the parameter `axis` works from these examples above.  The shape of an array is a tuple like `(3,1,2)`,   in which the first number denotes the axis 0,  and the second denotes the axis 1, and the last denotes the axis 2

if we sum the array in axis 0,  it will return a array with the shape `(1,1,2)` ,  it is like the array is compressed in the axis 0.

In most cases, we are dealing with 2D arrays , so if we sum the array in axis 0,  which means that we sum the each column of the array. So similarly,  if we sum the array in axis 1,  we are sum the array in each row.

```c
    def compute(self, a):
        ### BEGIN YOUR SOLUTION
        return array_api.sum(a, self.axes)
        ### END YOUR SOLUTION
```

- `BroadcastTo`: broadcast an array to a new shape (1 input, `shape` - tuple)

Some times we want to expand our matrix, for example,  if I have a matrix  with the shape of `(3,1,2)`,   and I need a matrix with the shape of `(3,3,2)`,  but there is one requirement which is that we want the two matrix are the same in axis 1. so in fact , we copy the axis 1 of the first matrix to 3 exactly same times to form the second matrix.

one thing need to remember is that the axis we want to broadcast needs to be 1 int the first matrix and the other axis need to be the same in both matrix. for example:

```python
a = np.array([1,2,3,4,5,6])
a.shape #(6,)
a = a.reshape(3,1,2)
a.shape #(3，1，2)
b=np.broadcast_to(a,(3,3,2))
b.shape #（3，3，2）
```

```python
    def compute(self, a):
        return array_api.broadcast_to(a, self.shape)
```

- `Reshape`: gives a new shape to an array without changing its data (1 input, `shape` - tuple)

```python
    def compute(self, a):
        ### BEGIN YOUR SOLUTION
        return array_api.reshape(a, self.shape)
        ### END YOUR SOLUTION
```

- `Negate`: numerical negative, element-wise (1 input)

```python
    def compute(self, a):
        ### BEGIN YOUR SOLUTION
        return array_api.negative(a)
        ### END YOUR SOLUTION
```

- `Transpose`: reverses the order of two axes (axis1, axis2), defaults to the last two axes (1 input, `axes` - tuple)

```python
    def compute(self, a):
        ### BEGIN YOUR SOLUTION
        if self.axes:
            return array_api.swapaxes(a, self.axes[0], self.axes[1])
        else:
            return array_api.swapaxes(a, a.ndim - 2, a.ndim - 1)
        ### END YOUR SOLUTION
```

# Question 2: Implementing backward computation 

In this task, we need to implement the gradient of each operators,  I only show some tricky ones, and others can be solved by the high school knowledge.

- The first one I want to talk about is the function `transpose`. How can we compute $\frac{\partial l}{\partial X}$,  If we already know $\frac{\partial l}{\partial X^T}$

The reason I think this is a little bit tricky is that We've never met a problem like this. But after you understand how we 

compute the derivatives of a matrix, you can understand this.

```c
    def gradient(self, out_grad, node):
        ### BEGIN YOUR SOLUTION
        return out_grad.transpose(self.axes)
        ### END YOUR SOLUTION
```

- Then, I'd like to talk about `summation`,   Let me give you a simple example and I believe that you can understand.

Now,  I have a  matrix with the shape `(3,2,2)`,  and we sum it in the axis 1 , so we get a result matrix with the shape `(3,1,2)` or `(3,2)`.  denote the old matrix as `A` and the new matrix as `B`  ,  so how can we compute $\frac{\partial l}{\partial A}$ if we already know $\frac{\partial l}{\partial B }$

one thing we are sure about is that the outcome should have the shape `(3,2,2)`, but now we have a matrix with the shape `(3,1,2)` or `(3,2)` (their the same thing). One thing I can tell you that the answer is just simply broadcast the $\frac{\partial l}{\partial B }$

to shape `(3,2,2)`,  but I have not came up with a good way to explain.

if the shape of `B `  is `(3,1,2)`,  we just broadcast it,  but how about `(3,2)`?

so first we copy the shape of `A`   as  `C`  and we denote every number in axis(the parameter of summation)  as `idx`,  we let 

`C[idx] = 1`,  then we can get a shape of `(3,1,2)`

```c
def gradient(self, out_grad, node):
        ### BEGIN YOUR SOLUTION
        new_shape = list(node.inputs[0].shape)
        axes = range(len(new_shape)) if self.axes is None else self.axes
        for axis in axes:
            new_shape[axis] = 1
        return out_grad.reshape(new_shape).broadcast_to(node.inputs[0].shape)
        ### END YOUR SOLUTION
```

- maybe the most abstract one is `broadst_to`.  

suppose that we have a matrix `A` and a matrix `B` ,   the shape of  `A` is `(3,1,2)` and that of `B ` is `(3,3,2)`,  which means that `B` is the result of broadcasting `A`,  so how can we compute $\frac{\partial l}{\partial A}$ if we know $\frac{\partial l}{\partial B}$

the answer is we sum `B` to let its shape become the same as `A`.

in most cases the shape of B and the shape of A have the same length, only differ in some positions but if `B` has the shape of (3,),  we can only broadcast it to `(x,3)`   instead of `(3,x)` 

```python
    def gradient(self, out_grad, node):
        ### BEGIN YOUR SOLUTION
        # 先按广播的维度求和，再reshape到原始shape
        # 一维张量默认是行向量，一维张量(x, )可以广播到(y, x)，不能广播到(x, y)
        a = node.inputs[0]
        if a.shape == self.shape:
            return (out_grad, )
        new_a_shape = [1] * (len(self.shape)-len(a.shape)) + list(a.shape)
        axes = tuple([i for i in range(len(self.shape)) if (i>=len(new_a_shape) or new_a_shape[i]!=self.shape[i])])
        grad_a = summation(out_grad, axes)
        grad_a = reshape(grad_a, a.shape)
        return (grad_a,)
```

# Question 3: Topological sort 

this task require us to find the topological sequence of all the node on a computational graph.  we can simply implement it by a dfs.

```python
def topo_sort_dfs(node, visited, topo_order):
    """Post-order DFS"""
    ### BEGIN YOUR SOLUTION
    if node in visited:
        return
    for in_node in node.inputs:
        topo_sort_dfs(in_node, visited, topo_order)
    topo_order.append(node)
    visited.add(node)
    ### END YOUR SOLUTION
```

# Question 4: Implementing reverse mode differentiation

so how can we implement the backward?

we iterate the reversed topological sort list,  we first compute the adjoint of this node($\frac{\partial l}{\partial x_i}$) then use the adjoint and  the operator of this node to compute the $\frac{\partial l}{\partial v_i} \frac{\partial v_i}{\partial v_j}$ for each input $v_j$ of $v_i$  ,  finally we can get the $\frac{\partial l}{\partial v_j}$ by sum all the  $\frac{\partial l}{\partial v_i} \frac{\partial v_i}{\partial v_j}$

```python
def compute_gradient_of_variables(output_tensor, out_grad):
    """Take gradient of output node with respect to each node in node_list.

    Store the computed result in the grad field of each Variable.
    """
    # a map from node to a list of gradient contributions from each output node
    node_to_output_grads_list: Dict[Tensor, List[Tensor]] = {}
    # Special note on initializing gradient of
    # We are really taking a derivative of the scalar reduce_sum(output_node)
    # instead of the vector output_node. But this is the common case for loss function.
    node_to_output_grads_list[output_tensor] = [out_grad]

    # Traverse graph in reverse topological order given the output_node that we are taking gradient wrt.
    reverse_topo_order = list(reversed(find_topo_sort([output_tensor])))

    ### BEGIN YOUR SOLUTION
    # print('revers topo order: ', len(reverse_topo_order))
    # for node in reverse_topo_order:
    #     print(id(node))
    for node in reverse_topo_order:
        sum_grad = None
        for partial_adjoint in node_to_output_grads_list[node]:
            if sum_grad is None:
                sum_grad = partial_adjoint
            else:
                sum_grad += partial_adjoint
        node.grad = sum_grad
        # print('grad shape: ', node.grad.shape)
        # if len(node.grad.shape) == 1:
        #     print(node.grad)
        # print(node.op)
        if not node.is_leaf():
            partial_adjoint_to_node = node.op.gradient(node.grad, node)
        # for i in range(len(node.inputs)):
        #     print('    inputs[{}]: {}  partial_adjoin: {}'.format(i, id(node.inputs[i]), partial_adjoint_to_node[i]))
        for i in range(len(node.inputs)):
            if node.inputs[i] in node_to_output_grads_list:
                node_to_output_grads_list[node.inputs[i]].append(partial_adjoint_to_node[i])
            else:
                node_to_output_grads_list[node.inputs[i]] = [partial_adjoint_to_node[i]]
      
    ### END YOUR SOLUTION
```

# Question 5: Softmax loss

this problem is relatively easy.

```python
return ndl.summation((ndl.log(ndl.summation(ndl.exp(Z), axes=(1,))) - ndl.summation(Z*y_one_hot, axes=(1,)))) / batch_size
```

# Question 6: SGD for a two-layer neural network

```python
def nn_epoch(X, y, W1, W2, lr = 0.1, batch=100):
    """ Run a single epoch of SGD for a two-layer neural network defined by the
    weights W1 and W2 (with no bias terms):
        logits = ReLU(X * W1) * W1
    The function should use the step size lr, and the specified batch size (and
    again, without randomizing the order of X).

    Args:
        X (np.ndarray[np.float32]): 2D input array of size
            (num_examples x input_dim).
        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)
        W1 (ndl.Tensor[np.float32]): 2D array of first layer weights, of shape
            (input_dim, hidden_dim)
        W2 (ndl.Tensor[np.float32]): 2D array of second layer weights, of shape
            (hidden_dim, num_classes)
        lr (float): step size (learning rate) for SGD
        batch (int): size of SGD mini-batch

    Returns:
        Tuple: (W1, W2)
            W1: ndl.Tensor[np.float32]
            W2: ndl.Tensor[np.float32]
    """

    ### BEGIN YOUR SOLUTION
    iterations = (y.size + batch - 1) // batch
    for i in range(iterations):
        x = ndl.Tensor(X[i * batch : (i+1) * batch, :])
        Z = ndl.relu(x.matmul(W1)).matmul(W2)
        yy = y[i * batch : (i+1) * batch]
        y_one_hot = np.zeros((batch, y.max() + 1))
        y_one_hot[np.arange(batch), yy] = 1
        y_one_hot = ndl.Tensor(y_one_hot)
        loss = softmax_loss(Z, y_one_hot)
        loss.backward()
        W1 = ndl.Tensor(W1.realize_cached_data() - lr * W1.grad.realize_cached_data())
        W2 = ndl.Tensor(W2.realize_cached_data() - lr * W2.grad.realize_cached_data())
    return W1, W2
    ### END YOUR SOLUTION
```

one thing I need to mention is that the update of `W1` and `W2` why we should first compute them as numpy array instead of just simply use $W1 = W1 - lr*W1$

If we compute `W1` like this,  we make it a tensor with two inputs and op, but actually `W1` should be a leaf node without op and inputs.

Besides, I think we can also use `W1.data` to compute this.